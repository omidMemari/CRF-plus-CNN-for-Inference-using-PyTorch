# -*- coding: utf-8 -*-
"""Copy of AML Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WyVn5SpVYixYsZzLKlBBab68xIM-4Zgm
"""

import torch
import torch.optim as optim
import torch.utils.data as data_utils
#from data_loader import get_dataset
import numpy as np
from crf import CRF

import gzip
import csv

class DataLoader:
    def __init__(self):
        data_path = '../data/letter.data.gz'
        lines = self._read(data_path)
        data, target = self._parse(lines)
        self.data, self.target = self._pad(data, target)

    @staticmethod
    def _read(filepath):
        with gzip.open(filepath, 'rt') as file_:
            reader = csv.reader(file_, delimiter='\t')
            lines = list(reader)
            return lines

    @staticmethod
    def _parse(lines):
        lines = sorted(lines, key=lambda x: int(x[0]))
        data, target = [], []
        next_ = None

        for line in lines:
            if not next_:
                data.append([])
                target.append([])
            else:
                assert next_ == int(line[0])
            next_ = int(line[2]) if int(line[2]) > -1 else None
            pixels = np.array([int(x) for x in line[6:134]])
            pixels = pixels.reshape((16, 8))
            data[-1].append(pixels)
            target[-1].append(line[1])
        return data, target

    @staticmethod
    def _pad(data, target):
        """
        Add padding to ensure word length is consistent
        """
        max_length = max(len(x) for x in target)
        padding = np.zeros((16, 8))
        data = [(x + ([padding] * (max_length - len(x)))) for x in data]
        #print("target")
        #print(target)
        target = [(x + ([''] * (max_length - len(x)))) for x in target]
        # print("target")
        # print(target)
        #print(type(target[0]))
        #raise
        #data = [x + ([padding] * 0) for x in data]
        #target = [x + ([''] * 0) for x in target]
        return np.array(data), np.array(target)

def get_dataset():
    dataset = DataLoader()

    # Flatten images into vectors.
    dataset.data = dataset.data.reshape(dataset.data.shape[:2] + (-1,))

    # One-hot encode targets.
    target = np.zeros(dataset.target.shape + (26,))
    # print(dataset.target)
    for index, letter in np.ndenumerate(dataset.target):
        # print(index)
        # print(letter)
        if letter:
            target[index][ord(letter) - ord('a')] = 1
    dataset.target = target

    # Shuffle order of examples.
    order = np.random.permutation(len(dataset.data))
    dataset.data = dataset.data[order]
    dataset.target = dataset.target[order]
    return dataset

# dataset = DataLoader()
# max_length = max(len(x) for x in dataset.target)
# target = [(x + ([''] * (max_length - len(x)))) for x in dataset.target]
# print("target")
# print(target)
# print("dataset.target")
# print(dataset.target)

# Tunable parameters
#batch_size = 256
batch_size = 256
num_epochs = 100
max_iters  = 100
print_iter = 25 # Prints results every n iterations
conv_shapes = [[1,64,128]] #

# Model parameters
input_dim = 128
embed_dim = 64
num_labels = 26
cuda = torch.cuda.is_available()

step = 0

# Fetch dataset
dataset = get_dataset()

split = int(0.01 * len(dataset.data)) # train-test split
#train_data, test_data = dataset.data[:split], dataset.data[split:]
#train_target, test_target = dataset.target[:split], dataset.target[split:]
train_data, test_data = dataset.data[:split], dataset.data[:split]
train_target, test_target = dataset.target[:split], dataset.target[:split]
  
batch_size = len(train_data)
print("batch size:")
print(batch_size)


# Instantiate the CRF model
crf = CRF(input_dim, embed_dim, conv_shapes, num_labels, batch_size)

print(list(crf.parameters()))

# Setup the optimizer
opt = optim.LBFGS(crf.parameters())

# Convert dataset into torch tensors
train = data_utils.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_target).long())
test = data_utils.TensorDataset(torch.tensor(test_data).float(), torch.tensor(test_target).long())

# Define train and test loaders
train_loader = data_utils.DataLoader(train,  # dataset to load from
                                         batch_size=batch_size,  # examples per batch (default: 1)
                                         shuffle=True,
                                         sampler=None,  # if a sampling method is specified, `shuffle` must be False
                                         num_workers=5,  # subprocesses to use for sampling
                                         pin_memory=False,  # whether to return an item pinned to GPU
                                         )

test_loader = data_utils.DataLoader(test,  # dataset to load from
                                        batch_size=batch_size,  # examples per batch (default: 1)
                                        shuffle=False,
                                        sampler=None,  # if a sampling method is specified, `shuffle` must be False
                                        num_workers=5,  # subprocesses to use for sampling
                                        pin_memory=False,  # whether to return an item pinned to GPU
                                        )
print('Loaded dataset... ')

from string import ascii_lowercase

for i in range(num_epochs):
    print("Processing epoch {}".format(i))
    dataset = get_dataset()
    #split = int(0.5 * len(dataset.data)) # train-test split
    # Now start training
    for i_batch, sample in enumerate(train_loader):
        train_X = sample[0]
        train_Y = sample[1]
        if cuda:
            train_X = train_X.cuda()
            train_Y = train_Y.cuda()
        
        def closure():
          print("in closure")
          opt.zero_grad()
          tr_loss = crf.loss(train_X, train_Y)
          tr_loss.backward()
          #crf.backward()
          #print(tr_loss)
          return tr_loss
        
        # compute loss, grads, updates:
        #opt.zero_grad() # clear the gradients
        #tr_loss = crf.loss(train_X, train_Y) # Obtain the loss for the optimizer to minimize
        #crf.backward() # Run backward pass and accumulate gradients
        opt.step(closure) # Perform optimization step (weight updates)
        
        # print to stdout occasionally:
        if step % print_iter == 0:
            #random_ixs = np.random.choice(test_data.shape[0], batch_size, replace=False)
            random_ixs = np.random.choice(test_data.shape[0], batch_size, replace=True)            
            test_X = test_data[random_ixs, :]
            test_Y = test_target[random_ixs, :]
            #test_Y = mapping[test_target[random_ixs, :]]

            # Convert to torch
            test_X = torch.from_numpy(test_X).float()
            test_Y = torch.from_numpy(test_Y).long()

            if cuda:
                test_X = test_X.cuda()
                test_Y = test_Y.cuda()
            #test_loss = crf.loss(test_X, test_Y)
            print("computing test accuracy")
            accuracy = crf.wordAccuracy(test_X, test_Y)
            print("accuracy:")
            print(accuracy)
            #print(step, tr_loss.data, test_loss.data,
            #           tr_loss.data / batch_size, test_loss.data / batch_size)

	    ##################################################################
	    # IMPLEMENT WORD-WISE AND LETTER-WISE ACCURACY HERE
	    ##################################################################

            #print(blah)

        step += 1
        if step > max_iters: raise StopIteration
    #del train, test

def testModelAccuracy():    
    random_ixs = np.random.choice(test_data.shape[0], batch_size, replace=True)            
    test_X = test_data[random_ixs, :]
    test_Y = test_target[random_ixs, :]
    #test_Y = mapping[test_target[random_ixs, :]]

    # Convert to torch
    test_X = torch.from_numpy(test_X).float()
    test_Y = torch.from_numpy(test_Y).long()

    if cuda:
        test_X = test_X.cuda()
        test_Y = test_Y.cuda()
        #test_loss = crf.loss(test_X, test_Y)
    print("computing test accuracy")
    accuracy = crf.wordAccuracy(test_X, test_Y)
    print("accuracy:")
    print(accuracy)

def testExistingModel():
    a = np.loadtxt("solution.txt")
    print(a.shape)
    W = a[:26*128].reshape(26,128)
    T = a[26*128:].reshape(26,26)
    print(W.shape)
    print(T.shape)

    random_ixs = np.random.choice(test_data.shape[0], batch_size, replace=True)            
    test_X = test_data[random_ixs, :]
    test_Y = test_target[random_ixs, :]
    #test_Y = mapping[test_target[random_ixs, :]]

    # Convert to torch
    test_X = torch.from_numpy(test_X).float()
    test_Y = torch.from_numpy(test_Y).long()

    if cuda:
        test_X = test_X.cuda()
        test_Y = test_Y.cuda()
        #test_loss = crf.loss(test_X, test_Y)
    print("computing test accuracy")
    accuracy = crf.computeModelAccuracy(test_X, test_Y, W, T)
    print("accuracy:")
    print(accuracy)


